{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/5887 [08:07<35:42:25, 21.92s/it]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m metadata[\u001b[33m'\u001b[39m\u001b[33mindices\u001b[39m\u001b[33m'\u001b[39m].append(idx)\n\u001b[32m     83\u001b[39m metadata[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m].append(law_title)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m metadata[\u001b[33m'\u001b[39m\u001b[33mkeywords\u001b[39m\u001b[33m'\u001b[39m].append(\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Implement keyword extraction\u001b[39;00m\n\u001b[32m     85\u001b[39m metadata[\u001b[33m'\u001b[39m\u001b[33mfield\u001b[39m\u001b[33m'\u001b[39m].append(extract_fields(content,fields))  \u001b[38;5;66;03m# Implement type classification\u001b[39;00m\n\u001b[32m     86\u001b[39m pbar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mextract_keywords\u001b[39m\u001b[34m(content)\u001b[39m\n\u001b[32m     33\u001b[39m     raw_output = raw_output.split(\u001b[33m'\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m```\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     37\u001b[39m result = json.loads(raw_output)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkeywords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "import ollama\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tqdm\n",
    "\n",
    "def filter_by_metadata(query, metadata_list):\n",
    "    \"\"\"\n",
    "    Returns indices of documents whose metadata matches the query.\n",
    "    It checks if the query contains words from 'chapter' or 'file' metadata.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    candidate_indices = []\n",
    "    for i, meta in enumerate(metadata_list):\n",
    "        chapter = meta.get(\"chapter\", \"\").lower()\n",
    "        file_name = meta.get(\"file\", \"\").lower()\n",
    "        # If the query includes either the chapter or file keywords, add the document index.\n",
    "        if chapter in query_lower or file_name in query_lower:\n",
    "            candidate_indices.append(i)\n",
    "    return candidate_indices\n",
    "\n",
    "\n",
    "def extract_keywords(content):\n",
    "    \"\"\"Extract legal keywords from content\"\"\"\n",
    "    response = ollama.generate(\n",
    "        model=\"deepseek-r1:32b\",\n",
    "        prompt=f\"Extract 5-7 legal keywords from this text:\\n{content}\\n Keywords:  Output as JSON\"\n",
    "    )\n",
    "    raw_text = response[\"response\"].strip()\n",
    "    raw_output = response[\"response\"].strip()\n",
    "    if '```json' in raw_output:\n",
    "        raw_output = raw_output.split('```json')[1].split('```')[0]\n",
    "   \n",
    "        \n",
    "    \n",
    "    result = json.loads(raw_output)\n",
    "    return result[\"keywords\"]\n",
    "\n",
    "\n",
    "def extract_fields(content,fields):\n",
    "    \"\"\"Extract legal keywords from content\"\"\"\n",
    "    response = ollama.generate(\n",
    "        model=\"deepseek-r1:32b\",\n",
    "        prompt=f\"Extract filed for :\\n{content}\\n from those fields:{fields} example output as JSON jsut 'field': (one the fields) \"\n",
    "    )\n",
    "    raw_text = response[\"response\"].strip()\n",
    "    raw_output = response[\"response\"].strip()\n",
    "    try :\n",
    "        if '```json' in raw_output:\n",
    "            raw_output = raw_output.split('```json')[1].split('```')[0]\n",
    "    \n",
    "            \n",
    "        \n",
    "        result = json.loads(raw_output)\n",
    "        return result[\"field\"]\n",
    "    except : \n",
    "        print(raw_output)\n",
    "\n",
    "\n",
    "# 1. Load Documents (Already Split)\n",
    "with open(\"C:\\dev\\Ramzey\\MOUSTACHAR\\RAG\\\\new_laws\\\\All_laws.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    laws = json.load(file)\n",
    "with open(\"C:\\dev\\Ramzey\\MOUSTACHAR\\RAG\\\\new_laws\\\\fields.txt\", \"r\",encoding=\"utf-8\") as file:\n",
    "    fields = file.read()\n",
    "\n",
    "\n",
    "documents = []\n",
    "metadata = {\n",
    "    'indices': [],\n",
    "    'title': [],\n",
    "    'keywords': [],\n",
    "    'field': []\n",
    "}\n",
    "pbar = tqdm.tqdm(total=len(laws))\n",
    "for idx, law in enumerate(laws):\n",
    "    law_title = list(law.keys())[0]\n",
    "    content = str(law[law_title])\n",
    "    \n",
    "    # Store document and metadata\n",
    "    documents.append(content)\n",
    "    metadata['indices'].append(idx)\n",
    "    metadata['title'].append(law_title)\n",
    "    metadata['keywords'].append(extract_keywords(content))  # Implement keyword extraction\n",
    "    metadata['field'].append(extract_fields(content,fields))  # Implement type classification\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "\n",
    "# 2. Compute Embeddings for Each Document Using Ollama\n",
    "all_embeddings = []\n",
    "for doc in documents:\n",
    "    response = ollama.embeddings(\n",
    "        model=\"snowflake-arctic-embed2\",\n",
    "        prompt=doc\n",
    "    )\n",
    "    all_embeddings.append(response[\"embedding\"])\n",
    "\n",
    "embeddings_np = np.array(all_embeddings, dtype=np.float32)\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "index.add(embeddings_np)\n",
    "print(f\"Faiss index size: {index.ntotal}\")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Querying Function Using Faiss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
